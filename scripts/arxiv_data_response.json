[
    {
        "entry_id": "\"http://arxiv.org/abs/1805.08355v1\"",
        "updated": "2018-05-22T02:12:33+00:00",
        "published": "2018-05-22T02:12:33+00:00",
        "title": "\"Opening the black box of deep learning\"",
        "authors": "[arxiv.Result.Author('Dian Lei'), arxiv.Result.Author('Xiaoxiao Chen'), arxiv.Result.Author('Jianfei Zhao')]",
        "summary": "\"The great success of deep learning shows that its technology contains\\nprofound truth, and understanding its internal mechanism not only has important\\nimplications for the development of its technology and effective application in\\nvarious fields, but also provides meaningful insights into the understanding of\\nhuman brain mechanism. At present, most of the theoretical research on deep\\nlearning is based on mathematics. This dissertation proposes that the neural\\nnetwork of deep learning is a physical system, examines deep learning from\\nthree different perspectives: microscopic, macroscopic, and physical world\\nviews, answers multiple theoretical puzzles in deep learning by using physics\\nprinciples. For example, from the perspective of quantum mechanics and\\nstatistical physics, this dissertation presents the calculation methods for\\nconvolution calculation, pooling, normalization, and Restricted Boltzmann\\nMachine, as well as the selection of cost functions, explains why deep learning\\nmust be deep, what characteristics are learned in deep learning, why\\nConvolutional Neural Networks do not have to be trained layer by layer, and the\\nlimitations of deep learning, etc., and proposes the theoretical direction and\\nbasis for the further development of deep learning now and in the future. The\\nbrilliance of physics flashes in deep learning, we try to establish the deep\\nlearning technology based on the scientific theory of physics.\"",
        "comment": "null",
        "journal_ref": "null",
        "doi": "null",
        "primary_category": "\"cs.LG\"",
        "categories": "[\"cs.LG\", \"stat.ML\"]",
        "links": "[arxiv.Result.Link('http://arxiv.org/abs/1805.08355v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.08355v1', title='pdf', rel='related', content_type=None)]",
        "pdf_url": "\"http://arxiv.org/pdf/1805.08355v1\"",
        "_raw": "{\"id\": \"http://arxiv.org/abs/1805.08355v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/1805.08355v1\", \"updated\": \"2018-05-22T02:12:33Z\", \"updated_parsed\": [2018, 5, 22, 2, 12, 33, 1, 142, 0], \"published\": \"2018-05-22T02:12:33Z\", \"published_parsed\": [2018, 5, 22, 2, 12, 33, 1, 142, 0], \"title\": \"Opening the black box of deep learning\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Opening the black box of deep learning\"}, \"summary\": \"The great success of deep learning shows that its technology contains\\nprofound truth, and understanding its internal mechanism not only has important\\nimplications for the development of its technology and effective application in\\nvarious fields, but also provides meaningful insights into the understanding of\\nhuman brain mechanism. At present, most of the theoretical research on deep\\nlearning is based on mathematics. This dissertation proposes that the neural\\nnetwork of deep learning is a physical system, examines deep learning from\\nthree different perspectives: microscopic, macroscopic, and physical world\\nviews, answers multiple theoretical puzzles in deep learning by using physics\\nprinciples. For example, from the perspective of quantum mechanics and\\nstatistical physics, this dissertation presents the calculation methods for\\nconvolution calculation, pooling, normalization, and Restricted Boltzmann\\nMachine, as well as the selection of cost functions, explains why deep learning\\nmust be deep, what characteristics are learned in deep learning, why\\nConvolutional Neural Networks do not have to be trained layer by layer, and the\\nlimitations of deep learning, etc., and proposes the theoretical direction and\\nbasis for the further development of deep learning now and in the future. The\\nbrilliance of physics flashes in deep learning, we try to establish the deep\\nlearning technology based on the scientific theory of physics.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"The great success of deep learning shows that its technology contains\\nprofound truth, and understanding its internal mechanism not only has important\\nimplications for the development of its technology and effective application in\\nvarious fields, but also provides meaningful insights into the understanding of\\nhuman brain mechanism. At present, most of the theoretical research on deep\\nlearning is based on mathematics. This dissertation proposes that the neural\\nnetwork of deep learning is a physical system, examines deep learning from\\nthree different perspectives: microscopic, macroscopic, and physical world\\nviews, answers multiple theoretical puzzles in deep learning by using physics\\nprinciples. For example, from the perspective of quantum mechanics and\\nstatistical physics, this dissertation presents the calculation methods for\\nconvolution calculation, pooling, normalization, and Restricted Boltzmann\\nMachine, as well as the selection of cost functions, explains why deep learning\\nmust be deep, what characteristics are learned in deep learning, why\\nConvolutional Neural Networks do not have to be trained layer by layer, and the\\nlimitations of deep learning, etc., and proposes the theoretical direction and\\nbasis for the further development of deep learning now and in the future. The\\nbrilliance of physics flashes in deep learning, we try to establish the deep\\nlearning technology based on the scientific theory of physics.\"}, \"authors\": [{\"name\": \"Dian Lei\"}, {\"name\": \"Xiaoxiao Chen\"}, {\"name\": \"Jianfei Zhao\"}], \"author_detail\": {\"name\": \"Jianfei Zhao\"}, \"author\": \"Jianfei Zhao\", \"links\": [{\"href\": \"http://arxiv.org/abs/1805.08355v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/1805.08355v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"stat.ML\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}"
    },
    {
        "entry_id": "\"http://arxiv.org/abs/1806.01756v1\"",
        "updated": "2018-06-05T15:50:30+00:00",
        "published": "2018-06-05T15:50:30+00:00",
        "title": "\"Concept-Oriented Deep Learning\"",
        "authors": "[arxiv.Result.Author('Daniel T Chang')]",
        "summary": "\"Concepts are the foundation of human deep learning, understanding, and\\nknowledge integration and transfer. We propose concept-oriented deep learning\\n(CODL) which extends (machine) deep learning with concept representations and\\nconceptual understanding capability. CODL addresses some of the major\\nlimitations of deep learning: interpretability, transferability, contextual\\nadaptation, and requirement for lots of labeled training data. We discuss the\\nmajor aspects of CODL including concept graph, concept representations, concept\\nexemplars, and concept representation learning systems supporting incremental\\nand continual learning.\"",
        "comment": "\"11 pages\"",
        "journal_ref": "null",
        "doi": "null",
        "primary_category": "\"cs.AI\"",
        "categories": "[\"cs.AI\"]",
        "links": "[arxiv.Result.Link('http://arxiv.org/abs/1806.01756v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.01756v1', title='pdf', rel='related', content_type=None)]",
        "pdf_url": "\"http://arxiv.org/pdf/1806.01756v1\"",
        "_raw": "{\"id\": \"http://arxiv.org/abs/1806.01756v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/1806.01756v1\", \"updated\": \"2018-06-05T15:50:30Z\", \"updated_parsed\": [2018, 6, 5, 15, 50, 30, 1, 156, 0], \"published\": \"2018-06-05T15:50:30Z\", \"published_parsed\": [2018, 6, 5, 15, 50, 30, 1, 156, 0], \"title\": \"Concept-Oriented Deep Learning\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Concept-Oriented Deep Learning\"}, \"summary\": \"Concepts are the foundation of human deep learning, understanding, and\\nknowledge integration and transfer. We propose concept-oriented deep learning\\n(CODL) which extends (machine) deep learning with concept representations and\\nconceptual understanding capability. CODL addresses some of the major\\nlimitations of deep learning: interpretability, transferability, contextual\\nadaptation, and requirement for lots of labeled training data. We discuss the\\nmajor aspects of CODL including concept graph, concept representations, concept\\nexemplars, and concept representation learning systems supporting incremental\\nand continual learning.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Concepts are the foundation of human deep learning, understanding, and\\nknowledge integration and transfer. We propose concept-oriented deep learning\\n(CODL) which extends (machine) deep learning with concept representations and\\nconceptual understanding capability. CODL addresses some of the major\\nlimitations of deep learning: interpretability, transferability, contextual\\nadaptation, and requirement for lots of labeled training data. We discuss the\\nmajor aspects of CODL including concept graph, concept representations, concept\\nexemplars, and concept representation learning systems supporting incremental\\nand continual learning.\"}, \"authors\": [{\"name\": \"Daniel T Chang\"}], \"author_detail\": {\"name\": \"Daniel T Chang\"}, \"author\": \"Daniel T Chang\", \"arxiv_comment\": \"11 pages\", \"links\": [{\"href\": \"http://arxiv.org/abs/1806.01756v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/1806.01756v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.AI\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.AI\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}"
    },
    {
        "entry_id": "\"http://arxiv.org/abs/1908.02130v1\"",
        "updated": "2019-07-30T16:57:38+00:00",
        "published": "2019-07-30T16:57:38+00:00",
        "title": "\"Deep learning research landscape & roadmap in a nutshell: past, present and future -- Towards deep cortical learning\"",
        "authors": "[arxiv.Result.Author('Aras R. Dargazany')]",
        "summary": "\"The past, present and future of deep learning is presented in this work.\\nGiven this landscape & roadmap, we predict that deep cortical learning will be\\nthe convergence of deep learning & cortical learning which builds an artificial\\ncortical column ultimately.\"",
        "comment": "null",
        "journal_ref": "null",
        "doi": "null",
        "primary_category": "\"cs.NE\"",
        "categories": "[\"cs.NE\", \"cs.LG\"]",
        "links": "[arxiv.Result.Link('http://arxiv.org/abs/1908.02130v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.02130v1', title='pdf', rel='related', content_type=None)]",
        "pdf_url": "\"http://arxiv.org/pdf/1908.02130v1\"",
        "_raw": "{\"id\": \"http://arxiv.org/abs/1908.02130v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/1908.02130v1\", \"updated\": \"2019-07-30T16:57:38Z\", \"updated_parsed\": [2019, 7, 30, 16, 57, 38, 1, 211, 0], \"published\": \"2019-07-30T16:57:38Z\", \"published_parsed\": [2019, 7, 30, 16, 57, 38, 1, 211, 0], \"title\": \"Deep learning research landscape & roadmap in a nutshell: past, present\\n  and future -- Towards deep cortical learning\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Deep learning research landscape & roadmap in a nutshell: past, present\\n  and future -- Towards deep cortical learning\"}, \"summary\": \"The past, present and future of deep learning is presented in this work.\\nGiven this landscape & roadmap, we predict that deep cortical learning will be\\nthe convergence of deep learning & cortical learning which builds an artificial\\ncortical column ultimately.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"The past, present and future of deep learning is presented in this work.\\nGiven this landscape & roadmap, we predict that deep cortical learning will be\\nthe convergence of deep learning & cortical learning which builds an artificial\\ncortical column ultimately.\"}, \"authors\": [{\"name\": \"Aras R. Dargazany\"}], \"author_detail\": {\"name\": \"Aras R. Dargazany\"}, \"author\": \"Aras R. Dargazany\", \"links\": [{\"href\": \"http://arxiv.org/abs/1908.02130v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/1908.02130v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.NE\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.NE\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}"
    },
    {
        "entry_id": "\"http://arxiv.org/abs/1812.05448v4\"",
        "updated": "2021-01-13T01:29:33+00:00",
        "published": "2018-11-08T07:59:23+00:00",
        "title": "\"A First Look at Deep Learning Apps on Smartphones\"",
        "authors": "[arxiv.Result.Author('Mengwei Xu'), arxiv.Result.Author('Jiawei Liu'), arxiv.Result.Author('Yuanqiang Liu'), arxiv.Result.Author('Felix Xiaozhu Lin'), arxiv.Result.Author('Yunxin Liu'), arxiv.Result.Author('Xuanzhe Liu')]",
        "summary": "\"We are in the dawn of deep learning explosion for smartphones. To bridge the\\ngap between research and practice, we present the first empirical study on\\n16,500 the most popular Android apps, demystifying how smartphone apps exploit\\ndeep learning in the wild. To this end, we build a new static tool that\\ndissects apps and analyzes their deep learning functions. Our study answers\\nthreefold questions: what are the early adopter apps of deep learning, what do\\nthey use deep learning for, and how do their deep learning models look like.\\nOur study has strong implications for app developers, smartphone vendors, and\\ndeep learning R\\\\&D. On one hand, our findings paint a promising picture of deep\\nlearning for smartphones, showing the prosperity of mobile deep learning\\nframeworks as well as the prosperity of apps building their cores atop deep\\nlearning. On the other hand, our findings urge optimizations on deep learning\\nmodels deployed on smartphones, the protection of these models, and validation\\nof research ideas on these models.\"",
        "comment": "null",
        "journal_ref": "null",
        "doi": "null",
        "primary_category": "\"cs.LG\"",
        "categories": "[\"cs.LG\", \"cs.CY\"]",
        "links": "[arxiv.Result.Link('http://arxiv.org/abs/1812.05448v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.05448v4', title='pdf', rel='related', content_type=None)]",
        "pdf_url": "\"http://arxiv.org/pdf/1812.05448v4\"",
        "_raw": "{\"id\": \"http://arxiv.org/abs/1812.05448v4\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/1812.05448v4\", \"updated\": \"2021-01-13T01:29:33Z\", \"updated_parsed\": [2021, 1, 13, 1, 29, 33, 2, 13, 0], \"published\": \"2018-11-08T07:59:23Z\", \"published_parsed\": [2018, 11, 8, 7, 59, 23, 3, 312, 0], \"title\": \"A First Look at Deep Learning Apps on Smartphones\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"A First Look at Deep Learning Apps on Smartphones\"}, \"summary\": \"We are in the dawn of deep learning explosion for smartphones. To bridge the\\ngap between research and practice, we present the first empirical study on\\n16,500 the most popular Android apps, demystifying how smartphone apps exploit\\ndeep learning in the wild. To this end, we build a new static tool that\\ndissects apps and analyzes their deep learning functions. Our study answers\\nthreefold questions: what are the early adopter apps of deep learning, what do\\nthey use deep learning for, and how do their deep learning models look like.\\nOur study has strong implications for app developers, smartphone vendors, and\\ndeep learning R\\\\&D. On one hand, our findings paint a promising picture of deep\\nlearning for smartphones, showing the prosperity of mobile deep learning\\nframeworks as well as the prosperity of apps building their cores atop deep\\nlearning. On the other hand, our findings urge optimizations on deep learning\\nmodels deployed on smartphones, the protection of these models, and validation\\nof research ideas on these models.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"We are in the dawn of deep learning explosion for smartphones. To bridge the\\ngap between research and practice, we present the first empirical study on\\n16,500 the most popular Android apps, demystifying how smartphone apps exploit\\ndeep learning in the wild. To this end, we build a new static tool that\\ndissects apps and analyzes their deep learning functions. Our study answers\\nthreefold questions: what are the early adopter apps of deep learning, what do\\nthey use deep learning for, and how do their deep learning models look like.\\nOur study has strong implications for app developers, smartphone vendors, and\\ndeep learning R\\\\&D. On one hand, our findings paint a promising picture of deep\\nlearning for smartphones, showing the prosperity of mobile deep learning\\nframeworks as well as the prosperity of apps building their cores atop deep\\nlearning. On the other hand, our findings urge optimizations on deep learning\\nmodels deployed on smartphones, the protection of these models, and validation\\nof research ideas on these models.\"}, \"authors\": [{\"name\": \"Mengwei Xu\"}, {\"name\": \"Jiawei Liu\"}, {\"name\": \"Yuanqiang Liu\"}, {\"name\": \"Felix Xiaozhu Lin\"}, {\"name\": \"Yunxin Liu\"}, {\"name\": \"Xuanzhe Liu\"}], \"author_detail\": {\"name\": \"Xuanzhe Liu\"}, \"author\": \"Xuanzhe Liu\", \"links\": [{\"href\": \"http://arxiv.org/abs/1812.05448v4\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/1812.05448v4\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"cs.CY\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}"
    },
    {
        "entry_id": "\"http://arxiv.org/abs/1901.02354v2\"",
        "updated": "2019-01-13T17:20:05+00:00",
        "published": "2019-01-06T14:32:45+00:00",
        "title": "\"Geometrization of deep networks for the interpretability of deep learning systems\"",
        "authors": "[arxiv.Result.Author('Xiao Dong'), arxiv.Result.Author('Ling Zhou')]",
        "summary": "\"How to understand deep learning systems remains an open problem. In this\\npaper we propose that the answer may lie in the geometrization of deep\\nnetworks. Geometrization is a bridge to connect physics, geometry, deep network\\nand quantum computation and this may result in a new scheme to reveal the rule\\nof the physical world. By comparing the geometry of image matching and deep\\nnetworks, we show that geometrization of deep networks can be used to\\nunderstand existing deep learning systems and it may also help to solve the\\ninterpretability problem of deep learning systems.\"",
        "comment": "\"9 pages, draft version\"",
        "journal_ref": "null",
        "doi": "null",
        "primary_category": "\"cs.LG\"",
        "categories": "[\"cs.LG\", \"cs.AI\", \"stat.ML\"]",
        "links": "[arxiv.Result.Link('http://arxiv.org/abs/1901.02354v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.02354v2', title='pdf', rel='related', content_type=None)]",
        "pdf_url": "\"http://arxiv.org/pdf/1901.02354v2\"",
        "_raw": "{\"id\": \"http://arxiv.org/abs/1901.02354v2\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/1901.02354v2\", \"updated\": \"2019-01-13T17:20:05Z\", \"updated_parsed\": [2019, 1, 13, 17, 20, 5, 6, 13, 0], \"published\": \"2019-01-06T14:32:45Z\", \"published_parsed\": [2019, 1, 6, 14, 32, 45, 6, 6, 0], \"title\": \"Geometrization of deep networks for the interpretability of deep\\n  learning systems\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Geometrization of deep networks for the interpretability of deep\\n  learning systems\"}, \"summary\": \"How to understand deep learning systems remains an open problem. In this\\npaper we propose that the answer may lie in the geometrization of deep\\nnetworks. Geometrization is a bridge to connect physics, geometry, deep network\\nand quantum computation and this may result in a new scheme to reveal the rule\\nof the physical world. By comparing the geometry of image matching and deep\\nnetworks, we show that geometrization of deep networks can be used to\\nunderstand existing deep learning systems and it may also help to solve the\\ninterpretability problem of deep learning systems.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"How to understand deep learning systems remains an open problem. In this\\npaper we propose that the answer may lie in the geometrization of deep\\nnetworks. Geometrization is a bridge to connect physics, geometry, deep network\\nand quantum computation and this may result in a new scheme to reveal the rule\\nof the physical world. By comparing the geometry of image matching and deep\\nnetworks, we show that geometrization of deep networks can be used to\\nunderstand existing deep learning systems and it may also help to solve the\\ninterpretability problem of deep learning systems.\"}, \"authors\": [{\"name\": \"Xiao Dong\"}, {\"name\": \"Ling Zhou\"}], \"author_detail\": {\"name\": \"Ling Zhou\"}, \"author\": \"Ling Zhou\", \"arxiv_comment\": \"9 pages, draft version\", \"links\": [{\"href\": \"http://arxiv.org/abs/1901.02354v2\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/1901.02354v2\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"cs.AI\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"stat.ML\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}"
    },
    {
        "entry_id": "\"http://arxiv.org/abs/1705.03921v1\"",
        "updated": "2017-05-10T18:52:26+00:00",
        "published": "2017-05-10T18:52:26+00:00",
        "title": "\"Why & When Deep Learning Works: Looking Inside Deep Learnings\"",
        "authors": "[arxiv.Result.Author('Ronny Ronen')]",
        "summary": "\"The Intel Collaborative Research Institute for Computational Intelligence\\n(ICRI-CI) has been heavily supporting Machine Learning and Deep Learning\\nresearch from its foundation in 2012. We have asked six leading ICRI-CI Deep\\nLearning researchers to address the challenge of \\\"Why & When Deep Learning\\nworks\\\", with the goal of looking inside Deep Learning, providing insights on\\nhow deep networks function, and uncovering key observations on their\\nexpressiveness, limitations, and potential. The output of this challenge\\nresulted in five papers that address different facets of deep learning. These\\ndifferent facets include a high-level understating of why and when deep\\nnetworks work (and do not work), the impact of geometry on the expressiveness\\nof deep networks, and making deep networks interpretable.\"",
        "comment": "\"This paper is the preface part of the \\\"Why & When Deep Learning works\\n  looking inside Deep Learning\\\" ICRI-CI paper bundle\"",
        "journal_ref": "null",
        "doi": "null",
        "primary_category": "\"cs.LG\"",
        "categories": "[\"cs.LG\"]",
        "links": "[arxiv.Result.Link('http://arxiv.org/abs/1705.03921v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1705.03921v1', title='pdf', rel='related', content_type=None)]",
        "pdf_url": "\"http://arxiv.org/pdf/1705.03921v1\"",
        "_raw": "{\"id\": \"http://arxiv.org/abs/1705.03921v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/1705.03921v1\", \"updated\": \"2017-05-10T18:52:26Z\", \"updated_parsed\": [2017, 5, 10, 18, 52, 26, 2, 130, 0], \"published\": \"2017-05-10T18:52:26Z\", \"published_parsed\": [2017, 5, 10, 18, 52, 26, 2, 130, 0], \"title\": \"Why & When Deep Learning Works: Looking Inside Deep Learnings\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Why & When Deep Learning Works: Looking Inside Deep Learnings\"}, \"summary\": \"The Intel Collaborative Research Institute for Computational Intelligence\\n(ICRI-CI) has been heavily supporting Machine Learning and Deep Learning\\nresearch from its foundation in 2012. We have asked six leading ICRI-CI Deep\\nLearning researchers to address the challenge of \\\"Why & When Deep Learning\\nworks\\\", with the goal of looking inside Deep Learning, providing insights on\\nhow deep networks function, and uncovering key observations on their\\nexpressiveness, limitations, and potential. The output of this challenge\\nresulted in five papers that address different facets of deep learning. These\\ndifferent facets include a high-level understating of why and when deep\\nnetworks work (and do not work), the impact of geometry on the expressiveness\\nof deep networks, and making deep networks interpretable.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"The Intel Collaborative Research Institute for Computational Intelligence\\n(ICRI-CI) has been heavily supporting Machine Learning and Deep Learning\\nresearch from its foundation in 2012. We have asked six leading ICRI-CI Deep\\nLearning researchers to address the challenge of \\\"Why & When Deep Learning\\nworks\\\", with the goal of looking inside Deep Learning, providing insights on\\nhow deep networks function, and uncovering key observations on their\\nexpressiveness, limitations, and potential. The output of this challenge\\nresulted in five papers that address different facets of deep learning. These\\ndifferent facets include a high-level understating of why and when deep\\nnetworks work (and do not work), the impact of geometry on the expressiveness\\nof deep networks, and making deep networks interpretable.\"}, \"authors\": [{\"name\": \"Ronny Ronen\"}], \"author_detail\": {\"name\": \"Ronny Ronen\"}, \"author\": \"Ronny Ronen\", \"arxiv_comment\": \"This paper is the preface part of the \\\"Why & When Deep Learning works\\n  looking inside Deep Learning\\\" ICRI-CI paper bundle\", \"links\": [{\"href\": \"http://arxiv.org/abs/1705.03921v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/1705.03921v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}"
    },
    {
        "entry_id": "\"http://arxiv.org/abs/2010.05125v2\"",
        "updated": "2021-12-02T02:39:50+00:00",
        "published": "2020-10-11T01:06:49+00:00",
        "title": "\"Learning Task-aware Robust Deep Learning Systems\"",
        "authors": "[arxiv.Result.Author('Keji Han'), arxiv.Result.Author('Yun Li'), arxiv.Result.Author('Xianzhong Long'), arxiv.Result.Author('Yao Ge')]",
        "summary": "\"Many works demonstrate that deep learning system is vulnerable to adversarial\\nattack. A deep learning system consists of two parts: the deep learning task\\nand the deep model. Nowadays, most existing works investigate the impact of the\\ndeep model on robustness of deep learning systems, ignoring the impact of the\\nlearning task. In this paper, we adopt the binary and interval label encoding\\nstrategy to redefine the classification task and design corresponding loss to\\nimprove robustness of the deep learning system. Our method can be viewed as\\nimproving the robustness of deep learning systems from both the learning task\\nand deep model. Experimental results demonstrate that our learning task-aware\\nmethod is much more robust than traditional classification while retaining the\\naccuracy.\"",
        "comment": "\"9 Pages\"",
        "journal_ref": "null",
        "doi": "null",
        "primary_category": "\"cs.LG\"",
        "categories": "[\"cs.LG\", \"cs.AI\", \"stat.ML\"]",
        "links": "[arxiv.Result.Link('http://arxiv.org/abs/2010.05125v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.05125v2', title='pdf', rel='related', content_type=None)]",
        "pdf_url": "\"http://arxiv.org/pdf/2010.05125v2\"",
        "_raw": "{\"id\": \"http://arxiv.org/abs/2010.05125v2\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2010.05125v2\", \"updated\": \"2021-12-02T02:39:50Z\", \"updated_parsed\": [2021, 12, 2, 2, 39, 50, 3, 336, 0], \"published\": \"2020-10-11T01:06:49Z\", \"published_parsed\": [2020, 10, 11, 1, 6, 49, 6, 285, 0], \"title\": \"Learning Task-aware Robust Deep Learning Systems\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Learning Task-aware Robust Deep Learning Systems\"}, \"summary\": \"Many works demonstrate that deep learning system is vulnerable to adversarial\\nattack. A deep learning system consists of two parts: the deep learning task\\nand the deep model. Nowadays, most existing works investigate the impact of the\\ndeep model on robustness of deep learning systems, ignoring the impact of the\\nlearning task. In this paper, we adopt the binary and interval label encoding\\nstrategy to redefine the classification task and design corresponding loss to\\nimprove robustness of the deep learning system. Our method can be viewed as\\nimproving the robustness of deep learning systems from both the learning task\\nand deep model. Experimental results demonstrate that our learning task-aware\\nmethod is much more robust than traditional classification while retaining the\\naccuracy.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Many works demonstrate that deep learning system is vulnerable to adversarial\\nattack. A deep learning system consists of two parts: the deep learning task\\nand the deep model. Nowadays, most existing works investigate the impact of the\\ndeep model on robustness of deep learning systems, ignoring the impact of the\\nlearning task. In this paper, we adopt the binary and interval label encoding\\nstrategy to redefine the classification task and design corresponding loss to\\nimprove robustness of the deep learning system. Our method can be viewed as\\nimproving the robustness of deep learning systems from both the learning task\\nand deep model. Experimental results demonstrate that our learning task-aware\\nmethod is much more robust than traditional classification while retaining the\\naccuracy.\"}, \"authors\": [{\"name\": \"Keji Han\"}, {\"name\": \"Yun Li\"}, {\"name\": \"Xianzhong Long\"}, {\"name\": \"Yao Ge\"}], \"author_detail\": {\"name\": \"Yao Ge\"}, \"author\": \"Yao Ge\", \"arxiv_comment\": \"9 Pages\", \"links\": [{\"href\": \"http://arxiv.org/abs/2010.05125v2\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2010.05125v2\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"cs.AI\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"stat.ML\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}"
    },
    {
        "entry_id": "\"http://arxiv.org/abs/1805.04825v1\"",
        "updated": "2018-05-13T06:01:39+00:00",
        "published": "2018-05-13T06:01:39+00:00",
        "title": "\"Deep Learning in Software Engineering\"",
        "authors": "[arxiv.Result.Author('Xiaochen Li'), arxiv.Result.Author('He Jiang'), arxiv.Result.Author('Zhilei Ren'), arxiv.Result.Author('Ge Li'), arxiv.Result.Author('Jingxuan Zhang')]",
        "summary": "\"Recent years, deep learning is increasingly prevalent in the field of\\nSoftware Engineering (SE). However, many open issues still remain to be\\ninvestigated. How do researchers integrate deep learning into SE problems?\\nWhich SE phases are facilitated by deep learning? Do practitioners benefit from\\ndeep learning? The answers help practitioners and researchers develop practical\\ndeep learning models for SE tasks. To answer these questions, we conduct a\\nbibliography analysis on 98 research papers in SE that use deep learning\\ntechniques. We find that 41 SE tasks in all SE phases have been facilitated by\\ndeep learning integrated solutions. In which, 84.7% papers only use standard\\ndeep learning models and their variants to solve SE problems. The\\npracticability becomes a concern in utilizing deep learning techniques. How to\\nimprove the effectiveness, efficiency, understandability, and testability of\\ndeep learning based solutions may attract more SE researchers in the future.\"",
        "comment": "\"10 pages, 4 figures\"",
        "journal_ref": "null",
        "doi": "null",
        "primary_category": "\"cs.SE\"",
        "categories": "[\"cs.SE\"]",
        "links": "[arxiv.Result.Link('http://arxiv.org/abs/1805.04825v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.04825v1', title='pdf', rel='related', content_type=None)]",
        "pdf_url": "\"http://arxiv.org/pdf/1805.04825v1\"",
        "_raw": "{\"id\": \"http://arxiv.org/abs/1805.04825v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/1805.04825v1\", \"updated\": \"2018-05-13T06:01:39Z\", \"updated_parsed\": [2018, 5, 13, 6, 1, 39, 6, 133, 0], \"published\": \"2018-05-13T06:01:39Z\", \"published_parsed\": [2018, 5, 13, 6, 1, 39, 6, 133, 0], \"title\": \"Deep Learning in Software Engineering\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Deep Learning in Software Engineering\"}, \"summary\": \"Recent years, deep learning is increasingly prevalent in the field of\\nSoftware Engineering (SE). However, many open issues still remain to be\\ninvestigated. How do researchers integrate deep learning into SE problems?\\nWhich SE phases are facilitated by deep learning? Do practitioners benefit from\\ndeep learning? The answers help practitioners and researchers develop practical\\ndeep learning models for SE tasks. To answer these questions, we conduct a\\nbibliography analysis on 98 research papers in SE that use deep learning\\ntechniques. We find that 41 SE tasks in all SE phases have been facilitated by\\ndeep learning integrated solutions. In which, 84.7% papers only use standard\\ndeep learning models and their variants to solve SE problems. The\\npracticability becomes a concern in utilizing deep learning techniques. How to\\nimprove the effectiveness, efficiency, understandability, and testability of\\ndeep learning based solutions may attract more SE researchers in the future.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Recent years, deep learning is increasingly prevalent in the field of\\nSoftware Engineering (SE). However, many open issues still remain to be\\ninvestigated. How do researchers integrate deep learning into SE problems?\\nWhich SE phases are facilitated by deep learning? Do practitioners benefit from\\ndeep learning? The answers help practitioners and researchers develop practical\\ndeep learning models for SE tasks. To answer these questions, we conduct a\\nbibliography analysis on 98 research papers in SE that use deep learning\\ntechniques. We find that 41 SE tasks in all SE phases have been facilitated by\\ndeep learning integrated solutions. In which, 84.7% papers only use standard\\ndeep learning models and their variants to solve SE problems. The\\npracticability becomes a concern in utilizing deep learning techniques. How to\\nimprove the effectiveness, efficiency, understandability, and testability of\\ndeep learning based solutions may attract more SE researchers in the future.\"}, \"authors\": [{\"name\": \"Xiaochen Li\"}, {\"name\": \"He Jiang\"}, {\"name\": \"Zhilei Ren\"}, {\"name\": \"Ge Li\"}, {\"name\": \"Jingxuan Zhang\"}], \"author_detail\": {\"name\": \"Jingxuan Zhang\"}, \"author\": \"Jingxuan Zhang\", \"arxiv_comment\": \"10 pages, 4 figures\", \"links\": [{\"href\": \"http://arxiv.org/abs/1805.04825v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/1805.04825v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.SE\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.SE\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}"
    },
    {
        "entry_id": "\"http://arxiv.org/abs/1901.09388v2\"",
        "updated": "2019-03-24T06:44:08+00:00",
        "published": "2019-01-27T14:54:51+00:00",
        "title": "\"Moving Deep Learning into Web Browser: How Far Can We Go?\"",
        "authors": "[arxiv.Result.Author('Yun Ma'), arxiv.Result.Author('Dongwei Xiang'), arxiv.Result.Author('Shuyu Zheng'), arxiv.Result.Author('Deyu Tian'), arxiv.Result.Author('Xuanzhe Liu')]",
        "summary": "\"Recently, several JavaScript-based deep learning frameworks have emerged,\\nmaking it possible to perform deep learning tasks directly in browsers.\\nHowever, little is known on what and how well we can do with these frameworks\\nfor deep learning in browsers. To bridge the knowledge gap, in this paper, we\\nconduct the first empirical study of deep learning in browsers. We survey 7\\nmost popular JavaScript-based deep learning frameworks, investigating to what\\nextent deep learning tasks have been supported in browsers so far. Then we\\nmeasure the performance of different frameworks when running different deep\\nlearning tasks. Finally, we dig out the performance gap between deep learning\\nin browsers and on native platforms by comparing the performance of\\nTensorFlow.js and TensorFlow in Python. Our findings could help application\\ndevelopers, deep-learning framework vendors and browser vendors to improve the\\nefficiency of deep learning in browsers.\"",
        "comment": "null",
        "journal_ref": "null",
        "doi": "null",
        "primary_category": "\"cs.SE\"",
        "categories": "[\"cs.SE\"]",
        "links": "[arxiv.Result.Link('http://arxiv.org/abs/1901.09388v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.09388v2', title='pdf', rel='related', content_type=None)]",
        "pdf_url": "\"http://arxiv.org/pdf/1901.09388v2\"",
        "_raw": "{\"id\": \"http://arxiv.org/abs/1901.09388v2\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/1901.09388v2\", \"updated\": \"2019-03-24T06:44:08Z\", \"updated_parsed\": [2019, 3, 24, 6, 44, 8, 6, 83, 0], \"published\": \"2019-01-27T14:54:51Z\", \"published_parsed\": [2019, 1, 27, 14, 54, 51, 6, 27, 0], \"title\": \"Moving Deep Learning into Web Browser: How Far Can We Go?\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Moving Deep Learning into Web Browser: How Far Can We Go?\"}, \"summary\": \"Recently, several JavaScript-based deep learning frameworks have emerged,\\nmaking it possible to perform deep learning tasks directly in browsers.\\nHowever, little is known on what and how well we can do with these frameworks\\nfor deep learning in browsers. To bridge the knowledge gap, in this paper, we\\nconduct the first empirical study of deep learning in browsers. We survey 7\\nmost popular JavaScript-based deep learning frameworks, investigating to what\\nextent deep learning tasks have been supported in browsers so far. Then we\\nmeasure the performance of different frameworks when running different deep\\nlearning tasks. Finally, we dig out the performance gap between deep learning\\nin browsers and on native platforms by comparing the performance of\\nTensorFlow.js and TensorFlow in Python. Our findings could help application\\ndevelopers, deep-learning framework vendors and browser vendors to improve the\\nefficiency of deep learning in browsers.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Recently, several JavaScript-based deep learning frameworks have emerged,\\nmaking it possible to perform deep learning tasks directly in browsers.\\nHowever, little is known on what and how well we can do with these frameworks\\nfor deep learning in browsers. To bridge the knowledge gap, in this paper, we\\nconduct the first empirical study of deep learning in browsers. We survey 7\\nmost popular JavaScript-based deep learning frameworks, investigating to what\\nextent deep learning tasks have been supported in browsers so far. Then we\\nmeasure the performance of different frameworks when running different deep\\nlearning tasks. Finally, we dig out the performance gap between deep learning\\nin browsers and on native platforms by comparing the performance of\\nTensorFlow.js and TensorFlow in Python. Our findings could help application\\ndevelopers, deep-learning framework vendors and browser vendors to improve the\\nefficiency of deep learning in browsers.\"}, \"authors\": [{\"name\": \"Yun Ma\"}, {\"name\": \"Dongwei Xiang\"}, {\"name\": \"Shuyu Zheng\"}, {\"name\": \"Deyu Tian\"}, {\"name\": \"Xuanzhe Liu\"}], \"author_detail\": {\"name\": \"Xuanzhe Liu\"}, \"author\": \"Xuanzhe Liu\", \"links\": [{\"href\": \"http://arxiv.org/abs/1901.09388v2\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/1901.09388v2\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.SE\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.SE\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}"
    },
    {
        "entry_id": "\"http://arxiv.org/abs/1602.00203v1\"",
        "updated": "2016-01-31T06:12:58+00:00",
        "published": "2016-01-31T06:12:58+00:00",
        "title": "\"Greedy Deep Dictionary Learning\"",
        "authors": "[arxiv.Result.Author('Snigdha Tariyal'), arxiv.Result.Author('Angshul Majumdar'), arxiv.Result.Author('Richa Singh'), arxiv.Result.Author('Mayank Vatsa')]",
        "summary": "\"In this work we propose a new deep learning tool called deep dictionary\\nlearning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\\na time. This requires solving a simple (shallow) dictionary learning problem,\\nthe solution to this is well known. We apply the proposed technique on some\\nbenchmark deep learning datasets. We compare our results with other deep\\nlearning tools like stacked autoencoder and deep belief network; and state of\\nthe art supervised dictionary learning tools like discriminative KSVD and label\\nconsistent KSVD. Our method yields better results than all.\"",
        "comment": "null",
        "journal_ref": "null",
        "doi": "null",
        "primary_category": "\"cs.LG\"",
        "categories": "[\"cs.LG\", \"cs.AI\", \"stat.ML\"]",
        "links": "[arxiv.Result.Link('http://arxiv.org/abs/1602.00203v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1602.00203v1', title='pdf', rel='related', content_type=None)]",
        "pdf_url": "\"http://arxiv.org/pdf/1602.00203v1\"",
        "_raw": "{\"id\": \"http://arxiv.org/abs/1602.00203v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/1602.00203v1\", \"updated\": \"2016-01-31T06:12:58Z\", \"updated_parsed\": [2016, 1, 31, 6, 12, 58, 6, 31, 0], \"published\": \"2016-01-31T06:12:58Z\", \"published_parsed\": [2016, 1, 31, 6, 12, 58, 6, 31, 0], \"title\": \"Greedy Deep Dictionary Learning\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Greedy Deep Dictionary Learning\"}, \"summary\": \"In this work we propose a new deep learning tool called deep dictionary\\nlearning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\\na time. This requires solving a simple (shallow) dictionary learning problem,\\nthe solution to this is well known. We apply the proposed technique on some\\nbenchmark deep learning datasets. We compare our results with other deep\\nlearning tools like stacked autoencoder and deep belief network; and state of\\nthe art supervised dictionary learning tools like discriminative KSVD and label\\nconsistent KSVD. Our method yields better results than all.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"In this work we propose a new deep learning tool called deep dictionary\\nlearning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\\na time. This requires solving a simple (shallow) dictionary learning problem,\\nthe solution to this is well known. We apply the proposed technique on some\\nbenchmark deep learning datasets. We compare our results with other deep\\nlearning tools like stacked autoencoder and deep belief network; and state of\\nthe art supervised dictionary learning tools like discriminative KSVD and label\\nconsistent KSVD. Our method yields better results than all.\"}, \"authors\": [{\"name\": \"Snigdha Tariyal\"}, {\"name\": \"Angshul Majumdar\"}, {\"name\": \"Richa Singh\"}, {\"name\": \"Mayank Vatsa\"}], \"author_detail\": {\"name\": \"Mayank Vatsa\"}, \"author\": \"Mayank Vatsa\", \"links\": [{\"href\": \"http://arxiv.org/abs/1602.00203v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/1602.00203v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"cs.AI\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"stat.ML\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}"
    }
]